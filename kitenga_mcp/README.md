# MCP Services Architecture

Clean, modular MCP server setup with persistent context per service. Each service has its own schema and tools to prevent context drops.

```
mcp_services/
â”œâ”€â”€ README.md (this file)
â”œâ”€â”€ tepo/              # Te PÅ backend service
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ schema.json
â”‚   â””â”€â”€ tools.json
â”œâ”€â”€ git/               # GitHub integration
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ schema.json
â”‚   â””â”€â”€ tools.json
â”œâ”€â”€ render/            # Render deployment service
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ schema.json
â”‚   â””â”€â”€ tools.json
â”œâ”€â”€ supabase/          # Supabase database service
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ schema.json
â”‚   â””â”€â”€ tools.json
â”œâ”€â”€ cloudflare/        # Cloudflare edge service
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ schema.json
â”‚   â””â”€â”€ tools.json
â”œâ”€â”€ shared/            # Shared utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ auth.py
â”‚   â”œâ”€â”€ context.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ config.yaml        # Main MCP config for Continue IDE
```

## Each Service Has:

1. **server.py** - MCP server implementation
2. **schema.json** - Input/output schema for tools (NO CONTEXT LOSS)
3. **tools.json** - Tool definitions with descriptions and examples

## Why This Works

- **Isolated Contexts**: Each service keeps its own schema in memory
- **No Loop Tunnels**: Schema is permanent per service, can't drop
- **Fast Recovery**: If context splits, each service has enough context to recover
- **Clean Execution**: Tools know exactly what they accept/return

## Running Locally

```bash
# Each server runs independently
python mcp_services/tepo/server.py
python mcp_services/git/server.py
python mcp_services/render/server.py
python mcp_services/supabase/server.py
python mcp_services/cloudflare/server.py
```

## Deploying to Production

All servers can run on Render as separate services, or bundled. Configuration in `config.yaml`.

## Continue IDE Integration

Add to `~/.continue/config.yaml`:

```yaml
mcpServers:
  tepo:
    command: python
    args: ["mcp_services/tepo/server.py"]
    env:
      TE_PO_BASE_URL: "https://your-render-url.onrender.com"

  git:
    command: python
    args: ["mcp_services/git/server.py"]
    env:
      GITHUB_TOKEN: ${GITHUB_TOKEN}

  render:
    command: python
    args: ["mcp_services/render/server.py"]
    env:
      RENDER_API_KEY: ${RENDER_API_KEY}

  supabase:
    command: python
    args: ["mcp_services/supabase/server.py"]
    env:
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_KEY: ${SUPABASE_KEY}

  cloudflare:
    command: python
    args: ["mcp_services/cloudflare/server.py"]
    env:
      CLOUDFLARE_API_KEY: ${CLOUDFLARE_API_KEY}
```

---

Each service is a self-contained carving - solid mauri fueled context that can't be dropped. ğŸ¤™

## Live Tooling Checklist

- **Manifest readiness** â€“ Run `./scripts/check_kitenga_manifest.sh` from the repo root to export `PIPELINE_TOKEN`, call `https://kitenga-main.onrender.com/tools/list`, and pipe the JSON through `jq`. If that succeeds, the manifest is available for GPT Builder / the Kitenga Whiro app.
- **Trigger a tool call** â€“ Use `./scripts/run_kitenga_tool_call.sh` with a JSON body (via STDIN or `-f`) to POST to `/tools/call`. The helper logs the bearer header for you, so you can test GET/POST operations (e.g. `{"domain":"kitenga","command":"kitenga_gpt_whisper","input":{"whisper":"Test"}}`).
- **Grab the trimmed schema** â€“ Fetch `https://kitenga-main.onrender.com/openapi-core.json` (or `/.well-known/openapi-core.json`) to expose the 30-path schema with the `servers` metadata that GPT Builder expects.
- **Point GPT at kitenga-main** â€“ Update GPT Builder/OpenAI apps to import `https://kitenga-main.onrender.com/openai_tools.json` (with `Authorization: Bearer $PIPELINE_TOKEN`) and use the `/openapi-core.json` schema so the same tool set is shared between the builder, the app, and your automation.

This keeps every live test running through `kitenga-main`, so the bearer token, stealth metadata, and vector logging stay centralized while you prep GPT to hit those endpoints.
